# -*- coding: utf-8 -*-
"""
Created on Mon Feb  6 10:49:59 2023

@author: jacob
"""
import os, glob
from datetime import datetime
import geopandas as gpd
import xarray as xr
import rioxarray as rxr

# Set paths to GDAL in virtual env to avoid proj.db errors
os.environ['PROJ_LIB'] = r"C:\Users\jacob\.conda\envs\yield\Library\share\proj"
os.environ['GDAL_DATA'] = r"C:\Users\jacob\.conda\envs\yield\Library\share"

def align_rasters(target, match):
    """
    This function reprojects, resamples, and aligns one DataArray or Dataset to another.
    It also fine tunes the pixel alignment after due to slight misalginments that
    occurs from errors in floating point precision.

    Parameters
    ----------
    target : Xarray: DataArray or Dataset
    match : Xarray: DataArray or Dataset

    Returns
    -------
    The target Xarray that is aligned.

    """
    transf = target.rio.reproject_match(match)
    transf = transf.assign_coords({
    "x": match.x,
    "y": match.y,
    })
    transf = transf.chunk(chunks="auto")
    return (transf)

def calc_anomalies(raw, baseline, proj, res, outDir, years=12):
    """
    Calculates anomalies given NDVI (raw) and NDVI baseline
    Assumes a dim named "time" to iterate through each time step
    
    Parameters
    ----------
    raw : Xarray Dataset
    baseline : Xarray Dataset
    years: Assumes monthly data. Specify # of years

    Returns
    -------
    Anomaly Dataset

    """
    
    rawname = list(raw.keys())[0]
    baselinename = list(baseline.keys())[0]
    
    template = raw[rawname].isel(time=1).rio.reproject(proj, resolution = res) - baseline[baselinename].isel(time=1).rio.reproject(proj, resolution = res)
    template = template.dropna('x', how='all', thresh=None).dropna('y', how='all', thresh=None)
    for i in list(range(0, years)):
        for j in list(range(0, len(baseline.time))):
            rawnum = (i*24) + j
            week = raw[rawname].isel(time=rawnum).time.values
            print (week)
            rawobs = raw[rawname].isel(time=rawnum).rio.reproject(proj, resolution = res)
            baseobs = baseline[baselinename].isel(time=j).rio.reproject(proj, resolution = res)
            ras = rawobs - baseobs
            ras = ras.rio.reproject_match(template)
            outname = outDir + f"//NDVIAnom_{week}" + ".tif"
            ras.rio.to_raster(outname)
    # Read data into xarray and write netcdf
    def get_dates(start = 2010, end = 2021 + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    # Open data 
    def preprocess(ras):
        return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]))
    anom = xr.open_mfdataset(outDir + '//NDVIAnom_*.tif', join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, parallel = True)
    anom['time'] = get_dates()
    return(anom)

def reproj_drop(stack, proj, res, dim1 = "x", dim2 = "y"):
    """
    Reprojects an Xarray dataset with one variable for the proj specified with
    the resolution specified. Also trims extra NAs outside the study area along
    the dims specified by dim1 and dim2.

    Parameters
    ----------
    stack : Xarray Dataset to alter
    proj : Proj wkt string
    res : Accepts a single number for both dims
    dim1 : Str, optional
        The longitude/x variable. The default is "x".
    dim2 : Str, optional
        The latitude/y variable. The default is "y".

    Returns
    -------
    Xarray DataArray that is reprojected and trimmed.

    """
    lyrlist = []
    name = list(stack.keys())[0]
    for i in list(range(0, int(len(stack.time)))):
        print (i)
        lyr = stack[name].isel(time=i).chunk({"x":500, "y":500})
        lyrlist.append(lyr.rio.reproject(proj, resolution = res).dropna(dim1, how='all', thresh=None).dropna(dim2, how='all', thresh=None))
    fstack=xr.concat(lyrlist, dim = 'time')
    return(fstack)

def export_array(stack, path):
    """
    Given a Xarray Dataset with one variable, this saves each layer as a file
    in the path specified. Currently needs to be updated to include any case
    other than saving NDVI anomalies from the yield project

    Parameters
    ----------
    stack : Xarray Dataset
    path : String to an output path

    Returns
    -------
    None.

    """
    name = list(stack.keys())[0]
    
    for i in list(range(0, int(len(stack.time)))):
        print (i)
        yearweek = str(stack[name].isel(time=i).time.values)
        outpath = path + r"\NDVIAnomalies_" + yearweek + ".tif"
        if not os.path.exists(outpath):
            lyr = stack[name].isel(time=i)
            lyr.rio.to_raster(outpath)

def stack(rawDir, clipshp, start = 2010, end = 2021):
    # Remove first two and last seven ZIP files
    files = glob.glob(rawDir + "//AgExtent.*.tif")
    def get_dates(start = start, end = end + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    # Open data 
    def preprocess(ras):
        return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]))
    raw = xr.open_mfdataset(files, join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, parallel = True)
    raw['time'] = get_dates()
    if end == 2010:
        raw["band_data"][5,:,:] = raw.to_array("band_data").isel(time=[4,6]).mean(dim="time").squeeze()

    # Clip
    pr_shp = gpd.read_file(clipshp)
    pr_shp = pr_shp.to_crs(raw.spatial_ref.crs_wkt)
    raw = raw.rio.clip(pr_shp.geometry.values, pr_shp.crs, from_disk=True)
    return (raw)

# def load_ndvi(path, start = 2010, end = 2021):
#     def preprocess(ras):
#         return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]))
#     def get_dates(start = start, end = end + 1): 
#         dates = []
#         for year in list(range(start, end)):
#             for week in list(range(15, 39)):
#                 if end > 2010:
#                     dates.append(str(year) + "W" + str(week))
#                 else:
#                     dates.append("W" + str(week))
#         return (dates)
#     ndvi = xr.open_mfdataset(glob.glob(path + "//NDVIAnom*.tif"), join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, parallel = True)
#     ndvi['time'] = get_dates()
#     return (ndvi)

def create_cdmstack_jw(cdmDir, ndvi, proj, outfile, start = 2010, end = 2021):
    """
    Description
    ----------
    This function takes the cdm files, reprojects and resamples to NDVI
    
    Parameters
    ----------
    cdmDir : Path to dir containing cdm files
    ccap : ndvi xarray

    Returns
    -------
    xarray stack with time series of cdm images aligned with CCAP
    """
    def get_dates(start = start, end = end + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    # Open data 
    def preprocess(ras, proj=proj, ndvi=ndvi):
        return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]).astype("uint8"))
    flist = []
    for year in [str(x) for x in list(range(10,22))]:
        for month in ["04", "05", "06", "07", "08", "09"]:
            f = glob.glob(cdmDir + f'\\*{year}{month}.tif')[0]
            if month in ["04", "09"]:
                flist.extend([f, f, f])
            elif month in ["05", "07"]:
                flist.extend([f, f, f, f])
            elif month in ["06", "08"]:
                flist.extend([f, f, f, f, f])
    cdmstack = xr.open_mfdataset(flist, join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, 
                                 parallel = True, chunks = "auto")
    cdmstack['time'] = get_dates()
    cdmstack = cdmstack.rio.reproject_match(ndvi)
    cdmstack = cdmstack.chunk(chunks="auto")
    cdmstack = cdmstack.rename({'band_data': "cdm"})
    cdmstack.to_netcdf(path = outfile, engine="netcdf4",
                       encoding = {"cdm": {"dtype": "int8", 'zlib': True}})
    return (cdmstack)

def create_yieldstack(shppath, outPath, templatePath, start = 2010, end = 2021):
    """
    Description
    ----------
    Get yield data into a stack of rasters (one each year)
    
    Parameters
    ----------
    shppath : Str
        Path to the yield shapefile
    template: str
        Path to template raster (ndvi)

    Returns
    -------
    Xarray of yield rasters for each year
    """
    from osgeo import gdal
    from osgeo.ogr import Open
    import numpy as np
    def rasterize_polygon(vec, ras, outRas, year):
        # Load and prepare
        ras_ds = gdal.Open(templatePath)
        vec_ds = Open(vec)
        shp = vec_ds.GetLayer()
        # Spatial info
        x_res = ras_ds.RasterXSize
        y_res = ras_ds.RasterYSize
        # Transform
        target_ds = gdal.GetDriverByName('GTiff').Create(outRas, x_res, y_res, 1, gdal.GDT_Float32)
        target_ds.SetGeoTransform(ras_ds.GetGeoTransform())
        target_ds.SetProjection(ras_ds.GetProjection())
        band = target_ds.GetRasterBand(1)
        NoData_value = -999999
        band.SetNoDataValue(NoData_value)
        band.FlushCache()
        gdal.RasterizeLayer(target_ds, [1], shp, options=[f"ATTRIBUTE=Yield{year}"])
        target_ds = None
    def get_dates(start = start, end = end + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    # Create folder and output rasterized yield
    crop = os.path.basename(shppath).split("Yield")[0]
    if not os.path.isdir(outPath):
        os.makedirs(outPath)
    for year in [str(x) for x in list(range(2010,2022))]:
        outRas = os.path.join(outPath, f'{crop}Yield{year}.tif')
        if not os.path.exists(outRas):
            rasterize_polygon(shppath, templatePath, outRas, year)
    # Get 24 of each raster to match temporal res of predictors
    flist = glob.glob(outPath + f"\\{crop}*.tif")*24
    flist.sort()
    # Prepare for stacking
    outfile = outPath + f"//{crop}Yield_20102021.nc"
    if not os.path.exists(outfile):
        r = rxr.open_rasterio(flist[0])
        nd = r.attrs['_FillValue']
        #proj = r.spatial_ref.crs_wkt
        #res = float(r.spatial_ref.GeoTransform.split(" ")[1])
        tempRas = rxr.open_rasterio(templatePath)
        proj = tempRas.spatial_ref.crs_wkt
        def preprocess(ras, nodata=nd, tempRas = tempRas):
            return(xr.where(ras != 0, ras, nd).drop('band').squeeze().expand_dims(time = [datetime.now()]))
        # Stack and postprocessing
        yieldstack = xr.open_mfdataset(flist, join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, 
                                     parallel = True, chunks = "auto")
        yieldstack['time'] = get_dates()
        crop_lower = crop.lower()
        yieldstack = yieldstack.rename({'band_data': f"{crop_lower}"})
        yieldstack = xr.where(yieldstack != 0, yieldstack, np.nan)
        yieldstack = yieldstack.rio.write_crs(proj)
        yieldstack[f'{crop_lower}'].rio.write_nodata(-999999.0, inplace=True)
    return(yieldstack)

def create_maskstack(crop, outDir, template, start = 2010, end = 2021):
    """
    Description
    ----------
    Get yieldmask data into a stack of rasters
    
    Parameters
    ----------
    crop : Str
        canola or wheat
    outDir: str
        Path to mask rasters

    Returns
    -------
    Xarray of yield mask rasters for each year
    """
    from datetime import datetime
    import numpy as np
    def get_dates(start = start, end = end + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    cropFiles = glob.glob(outDir + f"//{crop}mask_*")*24
    cropFiles.sort()
    def preprocess(ras):
        return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]))
    maskstack = xr.open_mfdataset(cropFiles, join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, 
                                 parallel = True, chunks = "auto")
    maskstack['time'] = get_dates()
    if crop == "canola":
        maskstack = xr.where(maskstack != 153, maskstack, 1,  keep_attrs=True) 
    elif crop == "wheat":
        maskstack = xr.where(maskstack != 140, maskstack, 1, keep_attrs=True)
        maskstack = xr.where(maskstack != 146, maskstack, 1, keep_attrs=True)
    maskstack = maskstack.rename({'band_data': f"{crop}"})
    return(maskstack)

def calc_yield_anomalies(ds, var):
    def get_dates(start = 2010, end = 2021 + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    t = ["2010W15","2011W15","2012W15","2013W15","2014W15","2015W15",
         "2016W15","2017W15","2018W15","2019W15","2020W15","2021W15"]
    da = ds[var].sel(time = t)
    #gb = ds.groupby('time.month')
    clim = da.mean(dim='time')
    anom = da - clim
    #anom.compute()
    dates = get_dates()
    for date in dates:
        if date == dates[0]:
            out = anom.sel(time = date)
        else:
            week = date[0:4] + "W15"
            arr = anom.sel(time = week)
            out = xr.concat([out, arr], dim = 'time')
    out['time'] = get_dates()
    out = out.to_dataset(name = 'ndvi')
    return(out)

def segment_images(ds, outPath, crop, yearweek):
    import os
    import numpy as np
    from skimage.segmentation import quickshift
    #import geopandas as gpd

    # Use first da to segment - they will all be the same
    var = 'ndvi'
    # Run segmentation on 2010 data first
    vals = ds[var].sel(time=yearweek).values.astype(np.float64) # range 0 to 24
    vals = np.where(np.isnan(vals), -100, vals)
    # Calculate the segments
    segments = quickshift(vals,
                          kernel_size=1,
                          convert2lab=False,
                          max_dist=2,
                          ratio=0.5,
                          channel_axis=-1)
    segments = segments.astype(int)
    print(segments.shape[0] * segments.shape[1])
    print(len(np.unique(segments)))
    # outRas.values = segments
    outRas = xr.DataArray(segments, coords={'y': ds.y,'x': ds.x}, dims=["y", "x"], attrs=ds[var].attrs)
    outRas = outRas.rio.write_crs(ds.spatial_ref.crs_wkt)
    outRas = outRas.rio.reproject_match(ds).rename("segments")
    outDir = os.path.dirname(outPath)
    year_char = yearweek[0:4]
    segmentPath = outDir + f"//{crop}segments{year_char}.tif"
    outRas.rio.to_raster(segmentPath)
    
    # Raster to polygon
    #proj = outRas.spatial_ref.crs_wkt
    #polygonize(segmentPath, outPath, proj)

# def polygonize(in_ras, out_shp, proj):
#     from osgeo import gdal, ogr, osr
#     #  get raster datasource
#     src_ds = gdal.Open(in_ras)
#     srcband = src_ds.GetRasterBand(1)
#     dst_layername = 'temp'
#     drv = ogr.GetDriverByName("ESRI Shapefile")
#     dst_ds = drv.CreateDataSource(out_shp)
#     # SR
#     sp_ref = osr.SpatialReference()
#     sp_ref.ImportFromProj4(proj)
#     # Create layer
#     dst_layer = dst_ds.CreateLayer(dst_layername, srs = sp_ref )
#     fld = ogr.FieldDefn("ID", ogr.OFTInteger)
#     dst_layer.CreateField(fld)
#     dst_field = dst_layer.GetLayerDefn().GetFieldIndex("ID")
#     gdal.Polygonize( srcband, None, dst_layer, dst_field, [], callback=None )
#     # return(gpd.read_file(out_shp))

def zonal_stats_table(data, zones, var, year_char, week_char, nodata):
    """
    Given a data array and zones array, this function calculates zonal stats

    Parameters
    ----------
    data : Numpy array (ravel() first)
        Numerical data to perform zonal stats
    zones : Numpy array (ravel() first)
        Integer data with same shape as data indicating zones
    var : String
        Variable name. Options are 'ndvi', 'cdm', and 'yield'
    year_char : String
        Year 
    week_char : String
        Week

    Returns
    -------
    Pandas data frame with zonal statistics for each zone IDs

    """
    from statistics import mode
    var_cap = var.capitalize()
    data_col = f'{var_cap}'
    # Create df and remove no data (-100)
    import pandas as pd
    df = pd.DataFrame({'ID': zones, 
                       'Year': [year_char]*len(zones),
                       'Week': [week_char]*len(zones),
                       data_col: data})
    df = df[df.ID != nodata]
    # Calculate zonal stats and add results as a column
    if var == 'ndvi':
        # df[f'Min{var_cap}'] = df.groupby('ID')[data_col].transform('min')
        # df[f'Max{var_cap}'] = df.groupby('ID')[data_col].transform('max')
        # df[f'Mean{var_cap}'] = df.groupby('ID')[data_col].transform('mean')
        # df[f'Median{var_cap}'] = df.groupby('ID')[data_col].transform('median')
        # df[f'Stdev{var_cap}'] = df.groupby('ID')[data_col].transform('std')
        # df['Count'] = df.groupby('ID')[data_col].transform('count')
        df = df.groupby(['ID', 'Year', 'Week']).agg(
            {data_col: ['min', 'max', 'mean', 'median', 'std', 'count']}
            ).droplevel(0, axis=1).rename(
                columns = {'min': f'Min{data_col}', 
                            'max': f'Max{data_col}',
                            'mean': f'Mean{data_col}',
                            'median': f'Median{data_col}',
                            'std': f'Stdev{data_col}',
                            'count': 'Count'}
                ).reset_index()
    elif var == 'cdm':
        df = df.groupby(['ID', 'Year', 'Week']).agg(
            {data_col: [mode]}
            ).droplevel(0, axis=1).rename(
                columns = {"mode": 'CDM'}
                ).astype(
                    {'CDM': 'int32'}
                    ).reset_index()
        #df[f'Mode{var_cap}'] = df.groupby('ID')[data_col].transform(mode)
        #df = df.astype({f'Mode{var_cap}': 'int32'})
    elif var == 'yield':
        df = df.groupby(['ID', 'Year', 'Week']).agg(
            {data_col: ['mean']}
            ).droplevel(0, axis=1).rename(
                columns = {"mean": data_col}
                ).reset_index()
        #df[f'Mean{var_cap}'] = df.groupby('ID')[data_col].transform('mean')

    else:
        raise ValueError("var must be one of: ndvi, cdm, or yield")
    # df = df.drop_duplicates(subset=['ID'])
    # df = df.drop(columns=[data_col])
    # df.reset_index(drop = True, inplace=True)
    return(df)
    
# def zonal_stats_table(zoneRas, ras, var, year_char, week_char):
#     import numpy as np
#     import pandas as pd
#     import statistics
#     if var == 'ndvi':
#         ids = list()
#         mn = list()
#         mx = list()
#         mean = list()
#         median = list()
#         stdev = list()
#         count = list()
#     elif var == 'cdm':
#         ids = list()
#         mode = list()
#     elif var == 'yield':
#         ids = list()
#         mean = list()
#     ras = ras.values
#     zoneRas = zoneRas.values
#     vals = np.unique(zoneRas)
#     rm = np.argwhere(vals == -100)
#     vals = np.delete(vals, rm)
#     for val in vals:
#         res = ras[zoneRas==val]
#         av = res.mean()
#         cnt = len(res)
#         if np.isnan(av) | cnt < 4:
#             continue
#         else:
#             if var == 'ndvi':
#                 ids.append(val)
#                 mn.append(res.min())
#                 mx.append(res.max())
#                 mean.append(av)
#                 median.append(np.median(res))
#                 stdev.append(res.std())
#                 count.append(cnt)
#             elif var == 'cdm':
#                 ids.append(val)
#                 md = max(statistics.multimode(res))
#                 mode.append(md)
#             elif var == 'yield':
#                 ids.append(val)
#                 mean.append(av)
#     var_cap = var.capitalize()
#     if var == 'ndvi':
#         d = {'ID': ids, 
#              'Year': [year_char]*len(ids),
#              'Week': [week_char]*len(ids),
#              f'Min{var_cap}': mn, 
#              f'Max{var_cap}': mx, 
#              f'Mean{var_cap}': mean,
#              f'Median{var_cap}': median,
#              f'STDEV{var_cap}': stdev, 
#              'Count': count}
#     elif var == 'cdm':
#         d = {'ID': ids,
#              'Year': [year_char]*len(ids),
#              'Week': [week_char]*len(ids),
#              f'Mode{var_cap}': mode}
#     elif var == 'yield':
#         d = {'ID': ids,
#              'Year': [year_char]*len(ids),
#              'Week': [week_char]*len(ids),
#              f'{var_cap}': mean}
#     df = pd.DataFrame(data = d)
#     return (df)

def get_maskweek(ds, year_char):
    import numpy as np
    # Get the non-NAs for each week into list
    times = [x for x in ds.time.values if year_char in x]
    lens = []
    for time in times:
        non_nas = np.count_nonzero(~np.isnan(ds['ndvi'].sel(time = time)))
        lens.append(non_nas)
    week = times[np.argmax(lens)]
    return (week)

def interp_df(df_year):
    import pandas as pd
    import numpy as np
    # Get in date format using first day of each week
    df_year['Date'] = pd.to_datetime(df_year.Year + df_year.Week + '-0', format = '%Y%W-%w')
    
    # Create multiindex for get unique combo of rows and date
    ids = list(np.unique(df_year.ID))
    rng = pd.date_range(df_year.Date[0].strftime('%m/%d/%Y'), periods=len(np.unique(df_year.Date)), freq='w')
    idx = pd.MultiIndex.from_product([ids, rng]) 
    df_year = df_year.set_index(['ID', 'Date'])
    
    # Create new df with all dates for each ID, populating rows we have data
    newdf = pd.DataFrame(columns = df_year.columns, index = idx)
    for col in df_year.columns:
        newdf[col] = df_year[col]
    newdf.Year = [x[1].strftime("%Y") for x in newdf.index]
    newdf.Week = [x[1].strftime("%W") for x in newdf.index]
    newdf.reset_index(inplace = True)
    newdf = newdf.rename(columns = {'level_0': 'ID', 'level_1': 'Date'})
    
    # Interpolate NaNs
    newdf['MinNdvi'] = newdf.groupby('ID', group_keys=False)['MinNdvi'].apply(lambda group: group.interpolate(method='linear'))
    newdf['MaxNdvi'] = newdf.groupby('ID', group_keys=False)['MaxNdvi'].apply(lambda group: group.interpolate(method='linear'))
    newdf['MeanNdvi'] = newdf.groupby('ID', group_keys=False)['MeanNdvi'].apply(lambda group: group.interpolate(method='linear'))
    newdf['MedianNdvi'] = newdf.groupby('ID', group_keys=False)['MedianNdvi'].apply(lambda group: group.interpolate(method='linear'))
    newdf['StdevNdvi'] = newdf.groupby('ID', group_keys=False)['StdevNdvi'].apply(lambda group: group.interpolate(method='linear'))
    # NaNs from first week should be backfilled
    newdf['MinNdvi'] = newdf.groupby('ID', group_keys=False)['MinNdvi'].apply(lambda group: group.fillna(method='bfill'))
    newdf['MaxNdvi'] = newdf.groupby('ID', group_keys=False)['MaxNdvi'].apply(lambda group: group.fillna(method='bfill'))
    newdf['MeanNdvi'] = newdf.groupby('ID', group_keys=False)['MeanNdvi'].apply(lambda group: group.fillna(method='bfill'))
    newdf['MedianNdvi'] = newdf.groupby('ID', group_keys=False)['MedianNdvi'].apply(lambda group: group.fillna(method='bfill'))
    newdf['StdevNdvi'] = newdf.groupby('ID', group_keys=False)['StdevNdvi'].apply(lambda group: group.fillna(method='bfill'))
    # Count and yield are bfilled AND ffilled
    newdf['Count'] = newdf.groupby('ID', group_keys=False)['Count'].apply(lambda group: group.fillna(method='ffill').fillna(method='bfill'))
    newdf['CDM'] = newdf.groupby('ID', group_keys=False)['CDM'].apply(lambda group: group.fillna(method='ffill').fillna(method='bfill'))
    newdf['Yield'] = newdf.groupby('ID', group_keys=False)['Yield'].apply(lambda group: group.fillna(method='ffill').fillna(method='bfill'))
    
    # drop date column and fix dtypes
    newdf.drop(columns = 'Date', inplace = True)
    newdf = newdf.astype({'CDM': 'int32', 'Count': 'int32'})
    return (newdf)

def agg_to_township(df):
    import numpy as np
    def mode(x):
        import statistics
        try:
            mode_value = statistics.mode(x)
            return mode_value
        except statistics.StatisticsError:
            return np.median(x)    # Group by cols
    mode2 = mode
    group_cols = ['TWP_ID', 'Year', 'Week']
    # Calculate weighted mean of field meanNDVI
    wmean = df.groupby(group_cols).apply(lambda x: np.average(x["MeanNdvi"], weights=x['Count'])).reset_index(name='WMeanNdvi')
    wstd = df.groupby(group_cols).apply(lambda x: np.sqrt(np.average((x['MeanNdvi'] - np.average(x['MeanNdvi'], weights=x['Count'])) ** 2, weights=x['Count']))).reset_index(name='WStdevNdvi')
    df_out = df.groupby(group_cols).agg(
        # std, range, IQR of MeanNDVI of ag fields in thw twp
        {"CDM": [mode],
         "MeanNdvi": [lambda x: x.max() - x.min(), 
                      lambda x: np.percentile(x, 75) - np.percentile(x, 25)],
         "Yield": [mode2]} 
        ).droplevel(0, axis=1).rename(
            columns = {
                '<lambda_0>': 'RangeNdvi',
                '<lambda_1>': 'IQRNdvi',
                'mode': 'ModeCDM',
                'mode2': 'Yield'}
            ).reset_index()
    df_out['WMeanNdvi'] = wmean['WMeanNdvi']
    df_out['WStdevNdvi'] = wstd['WStdevNdvi']
    df_out['CVNdvi'] = df_out['WStdevNdvi'] / df_out['WMeanNdvi']
    return (df_out)

def get_twp_ids(segments, twp, year):
    import geopandas as gpd
    import rasterio
    import numpy as np
    import pandas as pd
    
    def mode(x):
        import statistics
        try:
            mode_value = statistics.mode(x)
            return mode_value
        except statistics.StatisticsError:
            return np.median(x) 
    
    # Step 1: Load twp shapefile
    townships = gpd.read_file(twp)
    
    # Step 2: Reproject raster to township shp crs
    with rasterio.open(segments) as src:
        # Reproject the raster to the CRS of the GeoDataFrame
        raster_reprojected, transform = rasterio.warp.reproject(
            source=rasterio.band(src, 1),  # Use the first band of the raster
            destination=None,  # Reproject to a new ndarray
            src_transform=src.transform,
            src_crs=src.crs,
            dst_crs=townships.crs,
            resampling=rasterio.enums.Resampling.nearest
            )
        raster_array = raster_reprojected.squeeze()  # Assuming zone IDs are in the first band        

    # Step 3: Get the pixel coordinates of each zone ID
    rows, cols = np.where(raster_array >= 0)
    zone_ids = raster_array[rows, cols]
    pixel_coords = np.vstack((rows, cols)).T
    lon, lat = rasterio.transform.xy(transform, pixel_coords[:, 0], pixel_coords[:, 1])

    # Step 4: Create a pandas DataFrame of the points
    data = {'zone_id': zone_ids, 'longitude': lon, 'latitude': lat}
    df = pd.DataFrame(data)
    
    # Step 5: Convert DataFrame to a GeoDataFrame
    zone_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=townships.crs)
    
    # Step 6: Perform spatial join
    intersection_gdf = gpd.sjoin(zone_gdf, townships, how='inner', predicate='within')
    
    # Step 7: Subset the pandas DataFrame
    intersection_df = intersection_gdf[['zone_id', 'TWP_ID']]
    
    # Step 8: Get the TWP_ID belonging to each zone
    df = intersection_df.groupby('zone_id').agg({"TWP_ID": [mode]}).droplevel(0, axis=1).reset_index()
    
    # Step 9: Rename cols and add year
    df = df.rename(columns = {"zone_id": "ID", "mode": "TWP_ID"})
    df['Year'] = year
    # Print the DataFrame
    return(df)

def join_twp_ids(df_all, twp, crop, segdir):
    import pandas as pd
    yeardfs = []
    for year in list(range(2010, 2022)):
        year_char = str(year)
        segPath = segdir + f"//{crop}segments{year_char}.tif"
        df_ids = get_twp_ids(segPath, twp, year)
        print (df_ids)
        df_year = pd.merge(df_all, df_ids, on=['ID', 'Year'], how='inner')
        yeardfs.append(df_year)
    out = pd.concat(yeardfs)
    return(out)

def filter_preds(df, pred, week_str0, week_str1):
    next_dig = str(int(week_str0) + 1)
    #regex = f'{pred}_([' + f'{week_str0}' + '-3][' + f'{week_str1}' + '-9]|39)'
    regex = f'{pred}_(' + f'{week_str0}' + f'[{week_str1}-9]|[' + f'{next_dig}-9][0-9])'
    keepCols = list(df.loc[:,df.columns.str.match(regex)].columns)
    return (keepCols)

import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_probability as tfp

def nll(y_true, y_pred):
    return -y_pred.log_prob(y_true)

def normal_sp(params):
    import tensorflow_probability as tfp
    tfd = tfp.distributions
    return tfd.Normal(loc=params[:,0:1], scale=1e-3 + tf.math.softplus(0.05 * params[:,1:2]))# both parameters are learnable


@tf.autograph.experimental.do_not_convert
def custom_loss(y_true, y_pred):
    # Mean loss
    pred_mean = y_pred.mean()
    mse_loss = keras.losses.mean_squared_error(y_true, pred_mean)
    # Stddev loss
    pred_stddev = y_pred.stddev()
    obs_stddev = keras.backend.std(y_true, axis=0)
    stddev_diff = pred_stddev - obs_stddev
    uncertainty_loss = keras.losses.mean_squared_error(keras.backend.zeros_like(stddev_diff), stddev_diff)
    # Combined
    total_loss = mse_loss + uncertainty_loss
    return total_loss

@tf.autograph.experimental.do_not_convert
def weighted_loss(y_true, y_pred):
    # MSE error of the mean
    pred_mean = y_pred.mean()
    mse = tf.keras.losses.mean_squared_error(y_true, pred_mean)
    # Stddev loss
    pred_stddev = y_pred.stddev()
    uncertainty_loss = tf.keras.backend.mean(tf.keras.backend.square(pred_stddev - tf.keras.backend.std(y_true, axis=0)))
    # Combine the MSE and NLL terms with appropriate weights
    alpha = 0.95  # Weight for the MSE term
    beta = 0.05  # Weight for the IC term
    loss = mse * alpha + uncertainty_loss * beta
    return loss

def weighted_loss_nll(y_true, y_pred):
    # MSE
    pred_mean = y_pred.mean()
    mse = tf.keras.losses.mean_squared_error(y_true, pred_mean)

    # NLL
    nll = -tf.reduce_mean(y_pred.log_prob(y_true))

    # Combine the MSE and NLL terms with appropriate weights
    alpha = 0.6  # Weight for the MSE term
    beta = 0.4  # Weight for the NLL term
    loss = alpha * mse + beta * nll

    return loss

@tf.autograph.experimental.do_not_convert
def weighted_loss_intcov(y_true, y_pred):
    # Tuning for 68% CI
    # MSE error of the mean
    pred_mean = y_pred.mean()
    pred_stddev = y_pred.stddev()
    mse = tf.keras.losses.mean_squared_error(y_true, pred_mean)
    # Compute interval coverage
    lower_bound = pred_mean - pred_stddev  # Lower bound of the prediction interval (e.g., 95% CI)
    upper_bound = pred_mean + pred_stddev  # Upper bound of the prediction interval (e.g., 95% CI)
    interval_coverage = tf.reduce_mean(
        tf.cast(tf.logical_and(y_true >= lower_bound, y_true <= upper_bound), dtype=tf.float32)
    )

    # Calculate the difference between the interval coverage and the target coverage
    coverage_diff = interval_coverage - 0.68  # Target interval coverage of 95%
    # Scale the coverage difference to balance its contribution with MSE
    scaled_coverage_diff = 1000.0 * coverage_diff

    # Combine the MSE, interval coverage, and penalty terms with appropriate weights
    alpha = 0.6  # Weight for the MSE term
    beta = 0.4 # Weight for the interval coverage term
    #epsilon = 1e-7 # Add a small epsilon to avoid division by zero (inf loss)
    #loss = alpha * mse - beta * tf.math.log(interval_coverage + epsilon)
    loss = alpha * mse + beta * scaled_coverage_diff
    return loss

def interval_coverage(obs, mean, stdev, level = 0.95):
    import numpy as np
    from scipy.stats import norm
    obs = obs.squeeze()
    z = norm.ppf((1 + level) / 2)
    lower_bound = mean - stdev * z
    upper_bound = mean + stdev * z
    num_within_intervals = np.sum((obs >= lower_bound) & (obs <= upper_bound))
    coverage = num_within_intervals / len(obs) * 100
    return(round(coverage, 1))

import tensorflow.keras.backend as K
def interval_score_loss(y_true, y_pred, alpha=0.05):
    # Extract mean and standard deviation from predicted distribution
    pred_mean = y_pred.mean()
    pred_stddev = y_pred.stddev()
    # Compute lower and upper interval endpoints
    lower = pred_mean - pred_stddev
    upper = pred_mean + pred_stddev
    # Compute the interval score
    score = K.abs(upper - lower) + (2 / alpha) * (lower - y_true) * K.cast(y_true < lower, dtype='float32') + (2 / alpha) * (y_true - upper) * K.cast(y_true > upper, dtype='float32')
    # Compute the mean of the interval scores
    loss = K.mean(score)
    return loss


def build_model(hp):
    dataset_size = len(X_train_noID)
    kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) / tf.cast(dataset_size, dtype=tf.float32))
    model = keras.Sequential()
    if hp.Boolean("batchnorm"):
        model.add(keras.layers.BatchNormalization())
    model.add(tfp.layers.DenseFlipout(units=hp.Int("units1", min_value=32, max_value = 512, step = 32), kernel_divergence_fn=kl_divergence_function))
    model.add(keras.layers.ReLU())
    model.add(tfp.layers.DenseFlipout(units=hp.Int("units2", min_value=32, max_value = 512, step = 32), kernel_divergence_fn=kl_divergence_function))
    model.add(keras.layers.ReLU())
    model.add(tfp.layers.DenseFlipout(units=hp.Int("units3", min_value=32, max_value=512, step=32), kernel_divergence_fn=kl_divergence_function))
    model.add(keras.layers.ReLU())
    if hp.Boolean("dropout"):
        model.add(keras.layers.Dropout(rate=hp.Float("dropval", min_value=0.05, max_value=0.8, step=0.15)))
    model.add(keras.layers.DenseFlipout(units=2, kernel_divergence_fn=kl_divergence_function))
    model.add(tfp.layers.DistributionLambda(normal_sp))
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=yf.weighted_loss)
    return (model)

def testloss(y_true, y_pred):
    # Output two neurons, not distribution lamda
    # https://towardsdatascience.com/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f
    mu = y_pred[:, :1]  # first output neuron
    log_sig = y_pred[:, 1:]  # second output neuron
    sig = tf.exp(log_sig)  # undo the log

    return tf.reduce_mean(2 * log_sig + ((y_true - mu) / sig) ** 2)

def testloss_3(y_true, y_pred):
    # Output two neurons, not distribution lamda
    # https://towardsdatascience.com/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f
    mu = y_pred[:, :1]  # first output neuron
    log_sig = y_pred[:, 1:]  # second output neuron
    sig = tf.exp(log_sig)  # undo the log

    return tf.reduce_mean(3 * log_sig + ((y_true - mu) / sig) ** 2)

def testloss_15(y_true, y_pred):
    # Output two neurons, not distribution lamda
    # https://towardsdatascience.com/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f
    mu = y_pred[:, :1]  # first output neuron
    log_sig = y_pred[:, 1:]  # second output neuron
    sig = tf.exp(log_sig)  # undo the log

    return tf.reduce_mean(1.5 * log_sig + ((y_true - mu) / sig) ** 2)

# def gaussian_nll_loss(y_true, y_pred):
#     mu = y_pred[:, :1]
#     sig = y_pred[:, 1:]
#     var = tf.math.log(sig)  # Train on the ln so there are negs and pos (domain matching)
#     # add eps as small term for stability
#     mask = tf.less(var, 1e-6)
#     var = tf.where(mask, var + 1e-6, var)  # Add 1e-6 to the values where the mask is True
#     loss = tf.reduce_mean(0.5 * (var + (y_true - mu) ** 2 / sig))
#     return (loss)