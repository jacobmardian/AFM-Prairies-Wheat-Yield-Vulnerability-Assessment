import pandas as pd
import numpy as np

pd.set_option('display.max_columns', None)

# User params
predDir = r"D:\PhD\YieldProject\Output_Detrended_Ha\Predictions"

# all predictions - note that preds are sorted but other rows are not
df = pd.read_csv(predDir + "//AllPredictions.csv")
df_sorted = df.sort_values(by = ['Year', 'TWP_ID']).reset_index(drop = True)
#df_sorted = df_sorted[(df_sorted['TotalYield'] > 0)]
df_sorted = df_sorted[(df_sorted['Week'] > 17) & (df_sorted['TotalYield'] > 0)]

# Next, remove years where there is low yield but ND every week
df_sorted['Z'] = (df_sorted['TotalYield'] - df_sorted['MeanTotalYield']) / df_sorted['StdTotalYield']
def filter_condition(group):
    return all((group['TotalYield'] < 750) & (group['ModeCDM'] == 0) & (group['Z'] < -1.5))
groups_to_remove = df_sorted.groupby(['TWP_ID', 'Year']).filter(filter_condition)
remove_combinations = groups_to_remove[['TWP_ID', 'Year']].drop_duplicates()
df_merged = pd.merge(df_sorted, remove_combinations, on=['TWP_ID', 'Year'], how='outer', indicator=True)
df_filtered = df_merged[df_merged['_merge'] == 'left_only']
df_filtered = df_filtered.drop(columns=['_merge', 'Z'])

# First, find the 'TWP_ID's that have data for at least 9 years and filter to keep only those TWP_IDs
valid_twp_ids = df_filtered.groupby('TWP_ID')['Year'].nunique()
valid_twp_ids = valid_twp_ids[valid_twp_ids >= 9].index
df_filtered = df_filtered[df_filtered['TWP_ID'].isin(valid_twp_ids)]

# Convert from kg/acre to kg/acre
for colname in df_filtered.columns[7:12]:
    df_filtered[[colname]] = df_filtered[[colname]] * 2.47105
print (df_filtered)

df_filtered.to_csv(predDir + "//AllPredictions.csv", index = False)