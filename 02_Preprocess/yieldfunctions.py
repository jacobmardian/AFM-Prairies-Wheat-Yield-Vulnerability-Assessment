# -*- coding: utf-8 -*-
"""
Created on Mon Feb  6 10:49:59 2023

@author: jacob
"""
import os, glob
from datetime import datetime
import geopandas as gpd
import xarray as xr
import rioxarray as rxr

# Set paths to GDAL in virtual env to avoid proj.db errors
os.environ['PROJ_LIB'] = r"C:\Users\jacob\.conda\envs\yield\Library\share\proj"
os.environ['GDAL_DATA'] = r"C:\Users\jacob\.conda\envs\yield\Library\share"

def align_rasters(target, match):
    """
    This function reprojects, resamples, and aligns one DataArray or Dataset to another.
    It also fine tunes the pixel alignment after due to slight misalginments that
    occurs from errors in floating point precision.

    Parameters
    ----------
    target : Xarray: DataArray or Dataset
    match : Xarray: DataArray or Dataset

    Returns
    -------
    The target Xarray that is aligned.

    """
    transf = target.rio.reproject_match(match)
    transf = transf.assign_coords({
    "x": match.x,
    "y": match.y,
    })
    transf = transf.chunk(chunks="auto")
    return (transf)

def calc_anomalies(raw, baseline, proj, res, outDir, years=12):
    """
    Calculates anomalies given NDVI (raw) and NDVI baseline
    Assumes a dim named "time" to iterate through each time step
    
    Parameters
    ----------
    raw : Xarray Dataset
    baseline : Xarray Dataset
    years: Assumes monthly data. Specify # of years

    Returns
    -------
    Anomaly Dataset

    """
    
    rawname = list(raw.keys())[0]
    baselinename = list(baseline.keys())[0]
    
    template = raw[rawname].isel(time=1).rio.reproject(proj, resolution = res) - baseline[baselinename].isel(time=1).rio.reproject(proj, resolution = res)
    template = template.dropna('x', how='all', thresh=None).dropna('y', how='all', thresh=None)
    for i in list(range(0, years)):
        for j in list(range(0, len(baseline.time))):
            rawnum = (i*24) + j
            week = raw[rawname].isel(time=rawnum).time.values
            print (week)
            rawobs = raw[rawname].isel(time=rawnum).rio.reproject(proj, resolution = res)
            baseobs = baseline[baselinename].isel(time=j).rio.reproject(proj, resolution = res)
            ras = rawobs - baseobs
            ras = ras.rio.reproject_match(template)
            outname = outDir + f"//NDVIAnom_{week}" + ".tif"
            ras.rio.to_raster(outname)
    # Read data into xarray and write netcdf
    def get_dates(start = 2010, end = 2021 + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    # Open data 
    def preprocess(ras):
        return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]))
    anom = xr.open_mfdataset(outDir + '//NDVIAnom_*.tif', join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, parallel = True)
    anom['time'] = get_dates()
    return(anom)

def reproj_drop(stack, proj, res, dim1 = "x", dim2 = "y"):
    """
    Reprojects an Xarray dataset with one variable for the proj specified with
    the resolution specified. Also trims extra NAs outside the study area along
    the dims specified by dim1 and dim2.

    Parameters
    ----------
    stack : Xarray Dataset to alter
    proj : Proj wkt string
    res : Accepts a single number for both dims
    dim1 : Str, optional
        The longitude/x variable. The default is "x".
    dim2 : Str, optional
        The latitude/y variable. The default is "y".

    Returns
    -------
    Xarray DataArray that is reprojected and trimmed.

    """
    lyrlist = []
    name = list(stack.keys())[0]
    for i in list(range(0, int(len(stack.time)))):
        print (i)
        lyr = stack[name].isel(time=i).chunk({"x":500, "y":500})
        lyrlist.append(lyr.rio.reproject(proj, resolution = res).dropna(dim1, how='all', thresh=None).dropna(dim2, how='all', thresh=None))
    fstack=xr.concat(lyrlist, dim = 'time')
    return(fstack)

def export_array(stack, path):
    """
    Given a Xarray Dataset with one variable, this saves each layer as a file
    in the path specified. Currently needs to be updated to include any case
    other than saving NDVI anomalies from the yield project

    Parameters
    ----------
    stack : Xarray Dataset
    path : String to an output path

    Returns
    -------
    None.

    """
    name = list(stack.keys())[0]
    
    for i in list(range(0, int(len(stack.time)))):
        print (i)
        yearweek = str(stack[name].isel(time=i).time.values)
        outpath = path + r"\NDVIAnomalies_" + yearweek + ".tif"
        if not os.path.exists(outpath):
            lyr = stack[name].isel(time=i)
            lyr.rio.to_raster(outpath)

def stack(rawDir, clipshp, start = 2010, end = 2021):
    # Remove first two and last seven ZIP files
    files = glob.glob(rawDir + "//AgExtent.*.tif")
    def get_dates(start = start, end = end + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    # Open data 
    def preprocess(ras):
        return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]))
    raw = xr.open_mfdataset(files, join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, parallel = True)
    raw['time'] = get_dates()
    if end == 2010:
        raw["band_data"][5,:,:] = raw.to_array("band_data").isel(time=[4,6]).mean(dim="time").squeeze()

    # Clip
    pr_shp = gpd.read_file(clipshp)
    pr_shp = pr_shp.to_crs(raw.spatial_ref.crs_wkt)
    raw = raw.rio.clip(pr_shp.geometry.values, pr_shp.crs, from_disk=True)
    return (raw)

# def load_ndvi(path, start = 2010, end = 2021):
#     def preprocess(ras):
#         return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]))
#     def get_dates(start = start, end = end + 1): 
#         dates = []
#         for year in list(range(start, end)):
#             for week in list(range(15, 39)):
#                 if end > 2010:
#                     dates.append(str(year) + "W" + str(week))
#                 else:
#                     dates.append("W" + str(week))
#         return (dates)
#     ndvi = xr.open_mfdataset(glob.glob(path + "//NDVIAnom*.tif"), join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, parallel = True)
#     ndvi['time'] = get_dates()
#     return (ndvi)

def create_cdmstack_jw(cdmDir, ndvi, proj, outfile, start = 2010, end = 2021):
    """
    Description
    ----------
    This function takes the cdm files, reprojects and resamples to NDVI
    
    Parameters
    ----------
    cdmDir : Path to dir containing cdm files
    ccap : ndvi xarray

    Returns
    -------
    xarray stack with time series of cdm images aligned with CCAP
    """
    def get_dates(start = start, end = end + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    # Open data 
    def preprocess(ras, proj=proj, ndvi=ndvi):
        return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]).astype("uint8"))
    flist = []
    for year in [str(x) for x in list(range(10,22))]:
        for month in ["04", "05", "06", "07", "08", "09"]:
            f = glob.glob(cdmDir + f'\\*{year}{month}.tif')[0]
            if month in ["04", "09"]:
                flist.extend([f, f, f])
            elif month in ["05", "07"]:
                flist.extend([f, f, f, f])
            elif month in ["06", "08"]:
                flist.extend([f, f, f, f, f])
    cdmstack = xr.open_mfdataset(flist, join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, 
                                 parallel = True, chunks = "auto")
    cdmstack['time'] = get_dates()
    cdmstack = cdmstack.rio.reproject_match(ndvi)
    cdmstack = cdmstack.chunk(chunks="auto")
    cdmstack = cdmstack.rename({'band_data': "cdm"})
    cdmstack.to_netcdf(path = outfile, engine="netcdf4",
                       encoding = {"cdm": {"dtype": "int8", 'zlib': True}})
    return (cdmstack)

def create_yieldstack(shppath, outPath, templatePath, start = 2010, end = 2021):
    """
    Description
    ----------
    Get yield data into a stack of rasters (one each year)
    
    Parameters
    ----------
    shppath : Str
        Path to the yield shapefile
    template: str
        Path to template raster (ndvi)

    Returns
    -------
    Xarray of yield rasters for each year
    """
    from osgeo import gdal
    from osgeo.ogr import Open
    import numpy as np
    def rasterize_polygon(vec, ras, outRas, year):
        # Load and prepare
        ras_ds = gdal.Open(templatePath)
        vec_ds = Open(vec)
        shp = vec_ds.GetLayer()
        # Spatial info
        x_res = ras_ds.RasterXSize
        y_res = ras_ds.RasterYSize
        # Transform
        target_ds = gdal.GetDriverByName('GTiff').Create(outRas, x_res, y_res, 1, gdal.GDT_Float32)
        target_ds.SetGeoTransform(ras_ds.GetGeoTransform())
        target_ds.SetProjection(ras_ds.GetProjection())
        band = target_ds.GetRasterBand(1)
        NoData_value = -999999
        band.SetNoDataValue(NoData_value)
        band.FlushCache()
        gdal.RasterizeLayer(target_ds, [1], shp, options=[f"ATTRIBUTE=Yield{year}"])
        target_ds = None
    def get_dates(start = start, end = end + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    # Create folder and output rasterized yield
    crop = os.path.basename(shppath).split("Yield")[0]
    if not os.path.isdir(outPath):
        os.makedirs(outPath)
    for year in [str(x) for x in list(range(2010,2022))]:
        outRas = os.path.join(outPath, f'{crop}Yield{year}.tif')
        if not os.path.exists(outRas):
            rasterize_polygon(shppath, templatePath, outRas, year)
    # Get 24 of each raster to match temporal res of predictors
    flist = glob.glob(outPath + f"\\{crop}*.tif")*24
    flist.sort()
    # Prepare for stacking
    outfile = outPath + f"//{crop}Yield_20102021.nc"
    if not os.path.exists(outfile):
        r = rxr.open_rasterio(flist[0])
        nd = r.attrs['_FillValue']
        #proj = r.spatial_ref.crs_wkt
        #res = float(r.spatial_ref.GeoTransform.split(" ")[1])
        tempRas = rxr.open_rasterio(templatePath)
        proj = tempRas.spatial_ref.crs_wkt
        def preprocess(ras, nodata=nd, tempRas = tempRas):
            return(xr.where(ras != 0, ras, nd).drop('band').squeeze().expand_dims(time = [datetime.now()]))
        # Stack and postprocessing
        yieldstack = xr.open_mfdataset(flist, join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, 
                                     parallel = True, chunks = "auto")
        yieldstack['time'] = get_dates()
        crop_lower = crop.lower()
        yieldstack = yieldstack.rename({'band_data': f"{crop_lower}"})
        yieldstack = xr.where(yieldstack != 0, yieldstack, np.nan)
        yieldstack = yieldstack.rio.write_crs(proj)
        yieldstack[f'{crop_lower}'].rio.write_nodata(-999999.0, inplace=True)
    return(yieldstack)

def create_maskstack(crop, outDir, template, start = 2010, end = 2021):
    """
    Description
    ----------
    Get yieldmask data into a stack of rasters
    
    Parameters
    ----------
    crop : Str
        canola or wheat
    outDir: str
        Path to mask rasters

    Returns
    -------
    Xarray of yield mask rasters for each year
    """
    from datetime import datetime
    import numpy as np
    def get_dates(start = start, end = end + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    cropFiles = glob.glob(outDir + f"//{crop}mask_*")*24
    cropFiles.sort()
    def preprocess(ras):
        return(ras.drop('band').squeeze().expand_dims(time = [datetime.now()]))
    maskstack = xr.open_mfdataset(cropFiles, join = "left", concat_dim = 'time', combine='nested', preprocess = preprocess, 
                                 parallel = True, chunks = "auto")
    maskstack['time'] = get_dates()
    if crop == "canola":
        maskstack = xr.where(maskstack != 153, maskstack, 1,  keep_attrs=True) 
    elif crop == "wheat":
        maskstack = xr.where(maskstack != 140, maskstack, 1, keep_attrs=True)
        maskstack = xr.where(maskstack != 146, maskstack, 1, keep_attrs=True)
    maskstack = maskstack.rename({'band_data': f"{crop}"})
    return(maskstack)

def calc_yield_anomalies(ds, var):
    def get_dates(start = 2010, end = 2021 + 1): 
        dates = []
        for year in list(range(start, end)):
            for week in list(range(15, 39)):
                if end > 2010:
                    dates.append(str(year) + "W" + str(week))
                else:
                    dates.append("W" + str(week))
        return (dates)
    t = ["2010W15","2011W15","2012W15","2013W15","2014W15","2015W15",
         "2016W15","2017W15","2018W15","2019W15","2020W15","2021W15"]
    da = ds[var].sel(time = t)
    #gb = ds.groupby('time.month')
    clim = da.mean(dim='time')
    anom = da - clim
    #anom.compute()
    dates = get_dates()
    for date in dates:
        if date == dates[0]:
            out = anom.sel(time = date)
        else:
            week = date[0:4] + "W15"
            arr = anom.sel(time = week)
            out = xr.concat([out, arr], dim = 'time')
    out['time'] = get_dates()
    out = out.to_dataset(name = 'ndvi')
    return(out)

def segment_images(ds, outPath, crop, yearweek):
    import os
    import numpy as np
    from skimage.segmentation import quickshift
    #import geopandas as gpd

    # Use first da to segment - they will all be the same
    var = 'ndvi'
    # Run segmentation on 2010 data first
    vals = ds[var].sel(time=yearweek).values.astype(np.float64) # range 0 to 24
    vals = np.where(np.isnan(vals), -100, vals)
    # Calculate the segments
    segments = quickshift(vals,
                          kernel_size=1,
                          convert2lab=False,
                          max_dist=2,
                          ratio=0.5,
                          channel_axis=-1)
    segments = segments.astype(int)
    print(segments.shape[0] * segments.shape[1])
    print(len(np.unique(segments)))
    # outRas.values = segments
    outRas = xr.DataArray(segments, coords={'y': ds.y,'x': ds.x}, dims=["y", "x"], attrs=ds[var].attrs)
    outRas = outRas.rio.write_crs(ds.spatial_ref.crs_wkt)
    outRas = outRas.rio.reproject_match(ds).rename("segments")
    outDir = os.path.dirname(outPath)
    year_char = yearweek[0:4]
    segmentPath = outDir + f"//{crop}segments{year_char}.tif"
    outRas.rio.to_raster(segmentPath)
    
    # Raster to polygon
    #proj = outRas.spatial_ref.crs_wkt
    #polygonize(segmentPath, outPath, proj)

# def polygonize(in_ras, out_shp, proj):
#     from osgeo import gdal, ogr, osr
#     #  get raster datasource
#     src_ds = gdal.Open(in_ras)
#     srcband = src_ds.GetRasterBand(1)
#     dst_layername = 'temp'
#     drv = ogr.GetDriverByName("ESRI Shapefile")
#     dst_ds = drv.CreateDataSource(out_shp)
#     # SR
#     sp_ref = osr.SpatialReference()
#     sp_ref.ImportFromProj4(proj)
#     # Create layer
#     dst_layer = dst_ds.CreateLayer(dst_layername, srs = sp_ref )
#     fld = ogr.FieldDefn("ID", ogr.OFTInteger)
#     dst_layer.CreateField(fld)
#     dst_field = dst_layer.GetLayerDefn().GetFieldIndex("ID")
#     gdal.Polygonize( srcband, None, dst_layer, dst_field, [], callback=None )
#     # return(gpd.read_file(out_shp))

def zonal_stats_table(data, zones, var, year_char, week_char, nodata):
    """
    Given a data array and zones array, this function calculates zonal stats

    Parameters
    ----------
    data : Numpy array (ravel() first)
        Numerical data to perform zonal stats
    zones : Numpy array (ravel() first)
        Integer data with same shape as data indicating zones
    var : String
        Variable name. Options are 'ndvi', 'cdm', and 'yield'
    year_char : String
        Year 
    week_char : String
        Week

    Returns
    -------
    Pandas data frame with zonal statistics for each zone IDs

    """
    from statistics import mode
    var_cap = var.capitalize()
    data_col = f'{var_cap}'
    # Create df and remove no data (-100)
    import pandas as pd
    df = pd.DataFrame({'ID': zones, 
                       'Year': [year_char]*len(zones),
                       'Week': [week_char]*len(zones),
                       data_col: data})
    df = df[df.ID != nodata]
    # Calculate zonal stats and add results as a column
    if var == 'ndvi':
        # df[f'Min{var_cap}'] = df.groupby('ID')[data_col].transform('min')
        # df[f'Max{var_cap}'] = df.groupby('ID')[data_col].transform('max')
        # df[f'Mean{var_cap}'] = df.groupby('ID')[data_col].transform('mean')
        # df[f'Median{var_cap}'] = df.groupby('ID')[data_col].transform('median')
        # df[f'Stdev{var_cap}'] = df.groupby('ID')[data_col].transform('std')
        # df['Count'] = df.groupby('ID')[data_col].transform('count')
        df = df.groupby(['ID', 'Year', 'Week']).agg(
            {data_col: ['min', 'max', 'mean', 'median', 'std', 'count']}
            ).droplevel(0, axis=1).rename(
                columns = {'min': f'Min{data_col}', 
                            'max': f'Max{data_col}',
                            'mean': f'Mean{data_col}',
                            'median': f'Median{data_col}',
                            'std': f'Stdev{data_col}',
                            'count': 'Count'}
                ).reset_index()
    elif var == 'cdm':
        df = df.groupby(['ID', 'Year', 'Week']).agg(
            {data_col: [mode]}
            ).droplevel(0, axis=1).rename(
                columns = {"mode": 'CDM'}
                ).astype(
                    {'CDM': 'int32'}
                    ).reset_index()
        #df[f'Mode{var_cap}'] = df.groupby('ID')[data_col].transform(mode)
        #df = df.astype({f'Mode{var_cap}': 'int32'})
    elif var == 'yield':
        df = df.groupby(['ID', 'Year', 'Week']).agg(
            {data_col: ['mean']}
            ).droplevel(0, axis=1).rename(
                columns = {"mean": data_col}
                ).reset_index()
        #df[f'Mean{var_cap}'] = df.groupby('ID')[data_col].transform('mean')

    else:
        raise ValueError("var must be one of: ndvi, cdm, or yield")
    # df = df.drop_duplicates(subset=['ID'])
    # df = df.drop(columns=[data_col])
    # df.reset_index(drop = True, inplace=True)
    return(df)
    
# def zonal_stats_table(zoneRas, ras, var, year_char, week_char):
#     import numpy as np
#     import pandas as pd
#     import statistics
#     if var == 'ndvi':
#         ids = list()
#         mn = list()
#         mx = list()
#         mean = list()
#         median = list()
#         stdev = list()
#         count = list()
#     elif var == 'cdm':
#         ids = list()
#         mode = list()
#     elif var == 'yield':
#         ids = list()
#         mean = list()
#     ras = ras.values
#     zoneRas = zoneRas.values
#     vals = np.unique(zoneRas)
#     rm = np.argwhere(vals == -100)
#     vals = np.delete(vals, rm)
#     for val in vals:
#         res = ras[zoneRas==val]
#         av = res.mean()
#         cnt = len(res)
#         if np.isnan(av) | cnt < 4:
#             continue
#         else:
#             if var == 'ndvi':
#                 ids.append(val)
#                 mn.append(res.min())
#                 mx.append(res.max())
#                 mean.append(av)
#                 median.append(np.median(res))
#                 stdev.append(res.std())
#                 count.append(cnt)
#             elif var == 'cdm':
#                 ids.append(val)
#                 md = max(statistics.multimode(res))
#                 mode.append(md)
#             elif var == 'yield':
#                 ids.append(val)
#                 mean.append(av)
#     var_cap = var.capitalize()
#     if var == 'ndvi':
#         d = {'ID': ids, 
#              'Year': [year_char]*len(ids),
#              'Week': [week_char]*len(ids),
#              f'Min{var_cap}': mn, 
#              f'Max{var_cap}': mx, 
#              f'Mean{var_cap}': mean,
#              f'Median{var_cap}': median,
#              f'STDEV{var_cap}': stdev, 
#              'Count': count}
#     elif var == 'cdm':
#         d = {'ID': ids,
#              'Year': [year_char]*len(ids),
#              'Week': [week_char]*len(ids),
#              f'Mode{var_cap}': mode}
#     elif var == 'yield':
#         d = {'ID': ids,
#              'Year': [year_char]*len(ids),
#              'Week': [week_char]*len(ids),
#              f'{var_cap}': mean}
#     df = pd.DataFrame(data = d)
#     return (df)

def get_maskweek(ds, year_char):
    import numpy as np
    # Get the non-NAs for each week into list
    times = [x for x in ds.time.values if year_char in x]
    lens = []
    for time in times:
        non_nas = np.count_nonzero(~np.isnan(ds['ndvi'].sel(time = time)))
        lens.append(non_nas)
    week = times[np.argmax(lens)]
    return (week)

def interp_df(df_year):
    import pandas as pd
    import numpy as np
    # Get in date format using first day of each week
    df_year['Date'] = pd.to_datetime(df_year.Year + df_year.Week + '-0', format = '%Y%W-%w')
    
    # Create multiindex for get unique combo of rows and date
    ids = list(np.unique(df_year.ID))
    rng = pd.date_range(df_year.Date[0].strftime('%m/%d/%Y'), periods=len(np.unique(df_year.Date)), freq='w')
    idx = pd.MultiIndex.from_product([ids, rng]) 
    df_year = df_year.set_index(['ID', 'Date'])
    
    # Create new df with all dates for each ID, populating rows we have data
    newdf = pd.DataFrame(columns = df_year.columns, index = idx)
    for col in df_year.columns:
        newdf[col] = df_year[col]
    newdf.Year = [x[1].strftime("%Y") for x in newdf.index]
    newdf.Week = [x[1].strftime("%W") for x in newdf.index]
    newdf.reset_index(inplace = True)
    newdf = newdf.rename(columns = {'level_0': 'ID', 'level_1': 'Date'})
    
    # Interpolate NaNs
    newdf['MinNdvi'] = newdf.groupby('ID', group_keys=False)['MinNdvi'].apply(lambda group: group.interpolate(method='linear'))
    newdf['MaxNdvi'] = newdf.groupby('ID', group_keys=False)['MaxNdvi'].apply(lambda group: group.interpolate(method='linear'))
    newdf['MeanNdvi'] = newdf.groupby('ID', group_keys=False)['MeanNdvi'].apply(lambda group: group.interpolate(method='linear'))
    newdf['MedianNdvi'] = newdf.groupby('ID', group_keys=False)['MedianNdvi'].apply(lambda group: group.interpolate(method='linear'))
    newdf['StdevNdvi'] = newdf.groupby('ID', group_keys=False)['StdevNdvi'].apply(lambda group: group.interpolate(method='linear'))
    # NaNs from first week should be backfilled
    newdf['MinNdvi'] = newdf.groupby('ID', group_keys=False)['MinNdvi'].apply(lambda group: group.fillna(method='bfill'))
    newdf['MaxNdvi'] = newdf.groupby('ID', group_keys=False)['MaxNdvi'].apply(lambda group: group.fillna(method='bfill'))
    newdf['MeanNdvi'] = newdf.groupby('ID', group_keys=False)['MeanNdvi'].apply(lambda group: group.fillna(method='bfill'))
    newdf['MedianNdvi'] = newdf.groupby('ID', group_keys=False)['MedianNdvi'].apply(lambda group: group.fillna(method='bfill'))
    newdf['StdevNdvi'] = newdf.groupby('ID', group_keys=False)['StdevNdvi'].apply(lambda group: group.fillna(method='bfill'))
    # Count and yield are bfilled AND ffilled
    newdf['Count'] = newdf.groupby('ID', group_keys=False)['Count'].apply(lambda group: group.fillna(method='ffill').fillna(method='bfill'))
    newdf['CDM'] = newdf.groupby('ID', group_keys=False)['CDM'].apply(lambda group: group.fillna(method='ffill').fillna(method='bfill'))
    newdf['Yield'] = newdf.groupby('ID', group_keys=False)['Yield'].apply(lambda group: group.fillna(method='ffill').fillna(method='bfill'))
    
    # drop date column and fix dtypes
    newdf.drop(columns = 'Date', inplace = True)
    newdf = newdf.astype({'CDM': 'int32', 'Count': 'int32'})
    return (newdf)

def agg_to_township(df):
    import numpy as np
    def mode(x):
        import statistics
        try:
            mode_value = statistics.mode(x)
            return mode_value
        except statistics.StatisticsError:
            return np.median(x)    # Group by cols
    mode2 = mode
    group_cols = ['TWP_ID', 'Year', 'Week']
    # Calculate weighted mean of field meanNDVI
    wmean = df.groupby(group_cols).apply(lambda x: np.average(x["MeanNdvi"], weights=x['Count'])).reset_index(name='WMeanNdvi')
    wstd = df.groupby(group_cols).apply(lambda x: np.sqrt(np.average((x['MeanNdvi'] - np.average(x['MeanNdvi'], weights=x['Count'])) ** 2, weights=x['Count']))).reset_index(name='WStdevNdvi')
    df_out = df.groupby(group_cols).agg(
        # std, range, IQR of MeanNDVI of ag fields in thw twp
        {"CDM": [mode],
         "MeanNdvi": [lambda x: x.max() - x.min(), 
                      lambda x: np.percentile(x, 75) - np.percentile(x, 25)],
         "Yield": [mode2]} 
        ).droplevel(0, axis=1).rename(
            columns = {
                '<lambda_0>': 'RangeNdvi',
                '<lambda_1>': 'IQRNdvi',
                'mode': 'ModeCDM',
                'mode2': 'Yield'}
            ).reset_index()
    df_out['WMeanNdvi'] = wmean['WMeanNdvi']
    df_out['WStdevNdvi'] = wstd['WStdevNdvi']
    df_out['CVNdvi'] = df_out['WStdevNdvi'] / df_out['WMeanNdvi']
    return (df_out)

def get_twp_ids(segments, twp, year):
    import geopandas as gpd
    import rasterio
    import numpy as np
    import pandas as pd
    
    def mode(x):
        import statistics
        try:
            mode_value = statistics.mode(x)
            return mode_value
        except statistics.StatisticsError:
            return np.median(x) 
    
    # Step 1: Load twp shapefile
    townships = gpd.read_file(twp)
    
    # Step 2: Reproject raster to township shp crs
    with rasterio.open(segments) as src:
        # Reproject the raster to the CRS of the GeoDataFrame
        raster_reprojected, transform = rasterio.warp.reproject(
            source=rasterio.band(src, 1),  # Use the first band of the raster
            destination=None,  # Reproject to a new ndarray
            src_transform=src.transform,
            src_crs=src.crs,
            dst_crs=townships.crs,
            resampling=rasterio.enums.Resampling.nearest
            )
        raster_array = raster_reprojected.squeeze()  # Assuming zone IDs are in the first band        

    # Step 3: Get the pixel coordinates of each zone ID
    rows, cols = np.where(raster_array >= 0)
    zone_ids = raster_array[rows, cols]
    pixel_coords = np.vstack((rows, cols)).T
    lon, lat = rasterio.transform.xy(transform, pixel_coords[:, 0], pixel_coords[:, 1])

    # Step 4: Create a pandas DataFrame of the points
    data = {'zone_id': zone_ids, 'longitude': lon, 'latitude': lat}
    df = pd.DataFrame(data)
    
    # Step 5: Convert DataFrame to a GeoDataFrame
    zone_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=townships.crs)
    
    # Step 6: Perform spatial join
    intersection_gdf = gpd.sjoin(zone_gdf, townships, how='inner', predicate='within')
    
    # Step 7: Subset the pandas DataFrame
    intersection_df = intersection_gdf[['zone_id', 'TWP_ID']]
    
    # Step 8: Get the TWP_ID belonging to each zone
    df = intersection_df.groupby('zone_id').agg({"TWP_ID": [mode]}).droplevel(0, axis=1).reset_index()
    
    # Step 9: Rename cols and add year
    df = df.rename(columns = {"zone_id": "ID", "mode": "TWP_ID"})
    df['Year'] = year
    # Print the DataFrame
    return(df)

def join_twp_ids(df_all, twp, crop, segdir):
    import pandas as pd
    yeardfs = []
    for year in list(range(2010, 2022)):
        year_char = str(year)
        segPath = segdir + f"//{crop}segments{year_char}.tif"
        df_ids = get_twp_ids(segPath, twp, year)
        print (df_ids)
        df_year = pd.merge(df_all, df_ids, on=['ID', 'Year'], how='inner')
        yeardfs.append(df_year)
    out = pd.concat(yeardfs)
    return(out)

# def add_meanyield(df_all):
#     import pandas as pd
#     df_week = df_all[df_all['Week'] == 30]
#     df_week.reset_index(inplace=True, drop=True)
#     df_week['MeanYield'] = df_week.groupby(['TWP_ID'])['Yield'].transform(lambda x: [x.drop(i).mean() for i in x.index])
#     out = pd.merge(df_all, df_week[['TWP_ID', 'Year', 'MeanYield']],  how='left', left_on=['TWP_ID','Year'], right_on = ['TWP_ID','Year'])
#     return(out)

def fix_yield(df, crop, yieldDir):
    import pandas as pd
    crop_cap = crop.capitalize()
    # Join Yield data based on TWP_ID
    twp = gpd.read_file(os.path.join(yieldDir, f'{crop_cap}Yield_Township.shp'))
    twp_cols = ['TWP_ID'] + twp.columns[ + twp.columns.str.match('Yield')].tolist()
    df_out = pd.merge(df, twp[twp_cols], on=['TWP_ID'], how='inner')
    df_out['TotalYield'] = df_out.apply(lambda row: row['Yield' + str(row['Year'])], axis=1)
    df_out = df_out.drop(columns = df_out.columns[df_out.columns.str.match('Yield20')])
    # Calculate MeanTotalYield for all years EXCEPT the year of interest
    df_week = df_out[df_out['Week'] == 30]
    df_week.reset_index(inplace=True, drop=True)
    df_week['MeanTotalYield'] = df_week.groupby(['TWP_ID'])['TotalYield'].transform(lambda x: [x.drop(i).mean() for i in x.index])
    df_week['StdTotalYield'] = df_week.groupby(['TWP_ID'])['TotalYield'].transform(lambda x: [x.drop(i).std() for i in x.index])
    df_final = pd.merge(df_out, df_week[['TWP_ID', 'Year', 'MeanTotalYield', 'StdTotalYield']],  how='left', left_on=['TWP_ID','Year'], right_on = ['TWP_ID','Year'])
    df_final['YieldAnom'] = df_final['TotalYield'] - df_final['MeanTotalYield']
    df_final['YieldAnomStd'] = (df_final['TotalYield'] - df_final['MeanTotalYield']) / df_final['StdTotalYield']
    return (df_final)

def add_latlong(df, crop, yieldDir):
    import pandas as pd
    crop_cap = crop.capitalize()
    twp = gpd.read_file(os.path.join(yieldDir, f'{crop_cap}Yield_Township.shp'))
    df_out = pd.merge(df, twp[["TWP_ID", "Long", "Lat"]], on=['TWP_ID'], how='inner')
    # Reorder to make latlong the second and third
    cols = df_out.columns.tolist()
    latlong = cols[-2:]
    # Move the last two columns to the desired positions
    new_cols = (
        cols[:1] +  # First column
        latlong +  # Last two columns (to be placed at positions 3 and 4)
        cols[1:-2]  # Columns between the first and last two
    )
    # Rearrange the columns in the DataFrame
    df_out = df_out[new_cols]
    return (df_out)
